{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc89a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.file_converter import (\n",
    "    convert_doc_with_image_annotation,\n",
    "    chunk_markdown_by_headers,\n",
    "    export_to_md_with_image_ref,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c5f371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 21:31:35,098 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-14 21:31:38,644 - INFO - Going to convert document batch...\n",
      "2025-12-14 21:31:38,645 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 5bc1909d1e8430ba75d1da6a9ed3bb9e\n",
      "2025-12-14 21:31:38,696 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-14 21:31:38,715 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-14 21:31:38,735 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-14 21:31:38,752 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-14 21:31:38,940 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-14 21:31:38,941 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-14 21:31:40,379 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:40,442 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:40,471 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:40,686 [RapidOCR] download_file.py:60: File exists and is valid: D:\\DesktopFiles\\NextJsProjects\\BunxTest\\conversational-tutoring-system\\doc-processing\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:40,688 [RapidOCR] main.py:50: Using D:\\DesktopFiles\\NextJsProjects\\BunxTest\\conversational-tutoring-system\\doc-processing\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:42,576 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:42,577 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:42,607 [RapidOCR] download_file.py:60: File exists and is valid: D:\\DesktopFiles\\NextJsProjects\\BunxTest\\conversational-tutoring-system\\doc-processing\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:42,607 [RapidOCR] main.py:50: Using D:\\DesktopFiles\\NextJsProjects\\BunxTest\\conversational-tutoring-system\\doc-processing\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:42,763 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:42,763 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:43,107 [RapidOCR] download_file.py:60: File exists and is valid: D:\\DesktopFiles\\NextJsProjects\\BunxTest\\conversational-tutoring-system\\doc-processing\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-14 21:31:43,109 [RapidOCR] main.py:50: Using D:\\DesktopFiles\\NextJsProjects\\BunxTest\\conversational-tutoring-system\\doc-processing\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-14 21:31:43,652 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-14 21:31:43,691 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-14 21:31:43,717 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-14 21:31:43,797 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-14 21:31:45,059 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-14 21:31:45,061 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-14 21:31:45,380 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-14 21:31:46,705 - INFO - Processing document test-document-1.pdf\n",
      "2025-12-14 21:31:58,867 - INFO - Finished converting document test-document-1.pdf in 23.91 sec.\n"
     ]
    }
   ],
   "source": [
    "markdownDocument = convert_doc_with_image_annotation(\"docs/test-document-1.pdf\")\n",
    "# markdownString = markdownDocument.document.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887c8ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "## Docling Technical Report\n",
      "## Version 1.0  \n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar  \n",
      "AI4K Group, IBM Research RÂ¨ uschlikon, Switzerland\n",
      "## Abstract  \n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "## 1 Introduction  \n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.  \n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.  \n",
      "Here is what Docling delivers today:  \n",
      "- Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- Extracts metadata from the document, such as title, authors, references and language\n",
      "- Optionally applies OCR, e.g. for scanned PDFs\n",
      "- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- Can leverage different accelerators (GPU, MPS, etc).\n",
      "## 2 Getting Started  \n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.  \n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.  \n",
      "from docling.document\\_converter import DocumentConverter  \n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Large Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```  \n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "## 3 Processing pipeline  \n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "## 3.1 PDF backends  \n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive  \n",
      "1 see huggingface.co/ds4sd/docling-models/  \n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.  \n",
      "The image presents a detailed, step-by-step process for extracting and processing data from a PDF document. A PDF document, denoted by the Adobe logo, serves as the starting point. The document then undergoes parsing into individual PDF pages. These pages are then fed into a model pipeline, represented by a dotted line, which comprises four key stages:\n",
      "1. **OCR (Optical Character Recognition)**,\n",
      "2. **Layout Analysis**,\n",
      "3. **Table Structure**.\n",
      "Following the model pipeline, the document pages are reassembled and subjected to post-processing. The final output stage involves serializing the processed data into either JSON or Markdown format. This workflow outlines a structured approach to converting PDF documents into more usable and machine-readable formats.  \n",
      "<!-- image -->  \n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].  \n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "## 3.2 AI models  \n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "## Layout Analysis Model  \n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].  \n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "## Table Structure Recognition  \n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].  \n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "## OCR  \n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).  \n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "## 3.3 Assembly  \n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "## 3.4 Extensibility  \n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.  \n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "## 4 Performance  \n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.  \n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.  \n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and  \n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.  \n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.  \n",
      "| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "## 5 Applications  \n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "## 6 Future work and contributions  \n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.  \n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "## References  \n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n"
     ]
    }
   ],
   "source": [
    "chunkedMarkdownDocument = chunk_markdown_by_headers(markdownDocument.document.export_to_markdown())\n",
    "textsList = [ chunk.page_content for chunk in chunkedMarkdownDocument]\n",
    "for text in textsList:\n",
    "\tprint(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7c1375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image presents a detailed, step-by-step process for extracting and processing data from a PDF document. A PDF document, denoted by the Adobe logo, serves as the starting point. The document then undergoes parsing into individual PDF pages. These pages are then fed into a model pipeline, represented by a dotted line, which comprises four key stages: \\n1. **OCR (Optical Character Recognition)**, \\n2. **Layout Analysis**, \\n3. **Table Structure**. \\nFollowing the model pipeline, the document pages are reassembled and subjected to post-processing. The final output stage involves serializing the processed data into either JSON or Markdown format. This workflow outlines a structured approach to converting PDF documents into more usable and machine-readable formats.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16916\\1864593035.py:3: DeprecationWarning: Field `annotations` is deprecated; use `meta` instead.\n",
      "  imageAnnotationsList = [el.annotations[0].text for el in picturesList if len(el.annotations)>0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# markdownWithImageRef = export_to_md_with_image_ref(markdownDocument)\n",
    "picturesList = markdownDocument.document.pictures\n",
    "imageAnnotationsList = [el.annotations[0].text for el in picturesList if len(el.annotations)>0]\n",
    "print(imageAnnotationsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44eb3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating tables\n",
    "hasSeparatedTablesFromDocument = markdownDocument.document.tables\n",
    "tablesList = [table.export_to_html(doc=markdownDocument.document) for table in hasSeparatedTablesFromDocument]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21680662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text and Table Summaries\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2757b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptText = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "prefixPrompt = ChatPromptTemplate.from_template(promptText)\n",
    "model = ChatGroq(temperature=0.5,model=\"llama-3.1-8b-instant\")\n",
    "summarizerChain = {\"element\": lambda x:x} | prefixPrompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e397051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 21:32:19,910 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:19,917 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:19,944 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,220 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,254 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,284 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,433 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,485 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,613 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,698 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,730 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,842 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:20,979 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:21,106 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:21,160 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:21,246 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:21,252 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-12-14 21:32:21,258 - INFO - Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2025-12-14 21:32:21,338 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:22,503 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:22,622 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-12-14 21:32:22,626 - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds\n",
      "2025-12-14 21:32:27,949 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "textSummaries = summarizerChain.batch(textsList, {'max_concurrency': 3})\n",
    "\n",
    "tableSummaries = summarizerChain.batch(tablesList, {'max_concurrency': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textSummaries)\n",
    "print(tableSummaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b39b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 21:32:31,701 - INFO - HTTP Request: GET https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:32,597 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:33,267 - INFO - HTTP Request: GET https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/conversational_tutoring/exists \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:33,502 - INFO - HTTP Request: GET https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/conversational_tutoring \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 21:32:33,964 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Setting up the vector store\n",
    "import getpass\n",
    "import os\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "\tos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=30)\n",
    "\n",
    "vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "\n",
    "if not client.collection_exists(\"conversational_tutoring\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"conversational_tutoring\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "    )\n",
    "vectorStore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"conversational_tutoring\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be76e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "import uuid\n",
    "from langchain_core.stores import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "idKey = \"doc_id\"\n",
    "\n",
    "class MultiVectorRetriever(BaseRetriever): # langchain compatible implementation\n",
    "    vectorstore: Any\n",
    "    docstore: Any\n",
    "    id_key: str\n",
    "    k: int = 10\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        sub_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "\n",
    "        parent_ids = []\n",
    "        for doc in sub_docs:\n",
    "            if self.id_key not in doc.metadata:\n",
    "                raise ValueError(f\"Missing `{self.id_key}` in document metadata\")\n",
    "            parent_ids.append(doc.metadata[self.id_key])\n",
    "\n",
    "        # Deduplicate (preserve order)\n",
    "        seen = set()\n",
    "        parent_ids = [x for x in parent_ids if not (x in seen or seen.add(x))]\n",
    "\n",
    "        parent_docs = self.docstore.mget(parent_ids)\n",
    "        return [doc for doc in parent_docs if doc is not None]\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorStore,\n",
    "    docstore=store,\n",
    "    id_key=idKey,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9004d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 22:52:27,603 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 22:52:30,172 - INFO - HTTP Request: PUT https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/conversational_tutoring/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 22:52:30,617 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 22:52:30,890 - INFO - HTTP Request: PUT https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/conversational_tutoring/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 22:52:31,330 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 22:52:31,593 - INFO - HTTP Request: PUT https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/conversational_tutoring/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Add texts\n",
    "docIds = [str(uuid.uuid4()) for _ in textsList]\n",
    "summaryTexts = [\n",
    "    Document(page_content=summary, metadata={idKey: docIds[i]}) for i, summary in enumerate(textSummaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summaryTexts)\n",
    "retriever.docstore.mset(list(zip(docIds, textsList)))\n",
    "\n",
    "# Add tables\n",
    "tableIds = [str(uuid.uuid4()) for _ in tablesList]\n",
    "summaryTables = [\n",
    "    Document(page_content=summary, metadata={idKey: tableIds[i]}) for i, summary in enumerate(tableSummaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summaryTables)\n",
    "retriever.docstore.mset(list(zip(tableIds, tablesList)))\n",
    "\n",
    "# Add image summaries\n",
    "imgIds = [str(uuid.uuid4()) for _ in imageAnnotationsList]\n",
    "summaryImgs = [\n",
    "    Document(page_content=summary, metadata={idKey: imgIds[i]}) for i, summary in enumerate(imageAnnotationsList)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summaryImgs)\n",
    "retriever.docstore.mset(list(zip(imgIds, imageAnnotationsList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eccee8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 22:54:47,357 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-12-14 22:54:48,542 - INFO - HTTP Request: POST https://5e0668f4-54a2-4602-9c11-634e756c29c0.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/conversational_tutoring/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Testing out the retrieval\n",
    "docs = retriever._get_relevant_documents(\n",
    "    \"comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8772f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table><caption><div class=\"caption\">Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.</div></caption><tbody><tr><td>CPU</td><th>Thread budget</th><th colspan=\"3\">native backend</th><th colspan=\"3\">pypdfium backend</th></tr><tr><td></td><td></td><th>TTS</th><th>Pages/s</th><th>Mem</th><th>TTS</th><th>Pages/s</th><th>Mem</th></tr><tr><th>Apple M3 Max (16 cores)</th><td>4 16</td><td>177 s 167 s</td><td>1.27 1.34</td><td>6.20 GB</td><td>103 s 92 s</td><td>2.18 2.45</td><td>2.56 GB</td></tr><tr><th>Intel(R) Xeon E5-2690</th><td>4 16</td><td>375 s 244 s</td><td>0.60 0.92</td><td>6.16 GB</td><td>239 s 143 s</td><td>0.94 1.57</td><td>2.42 GB</td></tr></tbody></table>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "## 4 Performance  \n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.  \n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.  \n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and  \n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.  \n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.  \n",
      "| CPU                     | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|-------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                         |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max (16 cores) | 4 16            | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| Intel(R) Xeon E5-2690   | 4 16            | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "<!-- image -->\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "## References  \n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "## 5 Applications  \n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
